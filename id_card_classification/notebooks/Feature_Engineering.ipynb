{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Version: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "#importing required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage as s\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "import os\n",
    "import pandas as pd\n",
    "import keras\n",
    "import cv2\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Keras Version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path_to_img):\n",
    "    return cv2.imread(path_to_img)\n",
    "\n",
    "def get_blue_channel(path_to_img):\n",
    "    return load_img(path_to_img)[:,:,0].reshape(192,192,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "labels_csv = os.path.join(data_dir, 'gicsd_labels.csv')\n",
    "images_dir = os.path.join(data_dir, 'images')\n",
    "data_ids = [path for path in os.listdir(images_dir)]\n",
    "data_df = pd.read_csv(labels_csv)\n",
    "data_df.columns = ['IMAGE_FILENAME', 'LABEL']\n",
    "def rule(x):\n",
    "    if x['LABEL'].strip() == 'NO_VISIBILITY':\n",
    "        return '2'\n",
    "    if x['LABEL'].strip() == 'PARTIAL_VISIBILITY':\n",
    "        return '1'\n",
    "    if x['LABEL'].strip() == 'FULL_VISIBILITY':\n",
    "        return '0'\n",
    "    \n",
    "    return np.nan\n",
    "\n",
    "data_df['LABEL'] = data_df.apply(rule, axis = 1)\n",
    "# assert len(data_df) != len(data_ids)\n",
    "if len(data_df) != len(data_ids):\n",
    "    print('#################Some datasets are missing#####################')\n",
    "    \n",
    "# Here concluded that length is same for labels and images.\n",
    "\n",
    "#reading the image\n",
    "sample_img_id = data_ids[0]\n",
    "sample_img_path = os.path.join(images_dir, sample_img_id)\n",
    "img = get_blue_channel(sample_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, list_IDs, labels, image_path,\n",
    "                 to_fit=True, batch_size=32, dim=(192, 192),\n",
    "                 n_channels=3, n_classes=10, shuffle=False):\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.image_path = image_path\n",
    "        self.n_channels = n_channels\n",
    "        self.to_fit = to_fit\n",
    "        self.n_classes = n_classes\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.n = 0\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __next__(self):\n",
    "        # Get one batch of data\n",
    "        data = self.__getitem__(self.n)\n",
    "        # Batch index\n",
    "        self.n += 1\n",
    "\n",
    "        # If we have processed the entire dataset then\n",
    "        if self.n >= self.__len__():\n",
    "            self.on_epoch_end\n",
    "            self.n = 0\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return the number of batches of the dataset\n",
    "        return math.ceil(len(self.indexes)/self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X = self._generate_x(list_IDs_temp)\n",
    "\n",
    "        if self.to_fit:\n",
    "            y = self._generate_y(indexes)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        \n",
    "    def _generate_x(self, list_IDs_temp):\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, 1))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = self._load_blue_channel_image(os.path.join(self.image_path, ID))\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def _generate_y(self, indexes):\n",
    "\n",
    "        y = np.empty((self.batch_size, 1), dtype=int)\n",
    "\n",
    "        # Generate labels\n",
    "        for i,idx in enumerate(indexes):\n",
    "            # Store sample\n",
    "            temp = self.labels[idx]\n",
    "            if temp.strip() == '2':\n",
    "                z = 2\n",
    "            if temp.strip() == '1':\n",
    "                z = 1\n",
    "            if temp.strip() == '0':\n",
    "                z = 0\n",
    "            y[i,] = z\n",
    "\n",
    "        return keras.utils.to_categorical(y, num_classes = self.n_classes)\n",
    "    \n",
    "    def _load_blue_channel_image(self, image_path):\n",
    "        img = cv2.imread(image_path)\n",
    "        img = img[:,:,0].reshape(192,192,1)\n",
    "        img = img / 255\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 192, 192, 1)\n"
     ]
    }
   ],
   "source": [
    "data_idx = data_df['IMAGE_FILENAME'].tolist()\n",
    "labels_idx = data_df['LABEL'].tolist()\n",
    "\n",
    "gen = DataGenerator(data_idx, labels_idx,image_path = images_dir, n_classes = 3)\n",
    "\n",
    "image, y = next(gen)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/parth/anaconda3/envs/my_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/parth/anaconda3/envs/my_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/parth/anaconda3/envs/my_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/parth/anaconda3/envs/my_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/parth/anaconda3/envs/my_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/parth/anaconda3/envs/my_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/parth/anaconda3/envs/my_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/parth/anaconda3/envs/my_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/parth/anaconda3/envs/my_env/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 190, 190, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 190, 190, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 95, 95, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 95, 95, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 93, 93, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 93, 93, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 44, 44, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 44, 44, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 61952)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               31719936  \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 31,817,091\n",
      "Trainable params: 31,815,619\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(192, 192, 1)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(3, activation='softmax')) # 2 because we have cat and dog classes\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earlystop = EarlyStopping(patience=10)\n",
    "# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "#                                             patience=2, \n",
    "#                                             verbose=1, \n",
    "#                                             factor=0.5, \n",
    "#                                             min_lr=0.00001)\n",
    "# callbacks = [earlystop, learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs=3\n",
    "# batch_size = 32\n",
    "# history = model.fit_generator(\n",
    "#     gen,\n",
    "#     epochs=epochs,\n",
    "#     steps_per_epoch=len(data_ids)//batch_size,\n",
    "#     callbacks=callbacks\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
